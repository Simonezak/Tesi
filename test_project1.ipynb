{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc333225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# loader_features.py\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import wntr\n",
    "\n",
    "def _safe_float(x, default=None):\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return default\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def load_wdn(inp_path: str, use_length_as_weight: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, nx.Graph]:\n",
    "    \"\"\"\n",
    "    Carica il .inp WNTR e costruisce:\n",
    "      - nodes_df: feature base per nodo (elevation, base_demand, coords, node_type)\n",
    "      - links_df: feature base per arco (from,to,length,diameter,roughness,status,link_type)\n",
    "      - G: grafo NetworkX non orientato con 'weight' (di default = length)\n",
    "    \"\"\"\n",
    "    p = Path(inp_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"File .inp non trovato: {inp_path}\")\n",
    "\n",
    "    wn = wntr.network.WaterNetworkModel(inp_path)\n",
    "\n",
    "    all_nodes = pd.Index(wn.node_name_list, name=\"node_name\")\n",
    "    def q_node(attr: str) -> pd.Series:\n",
    "        return wn.query_node_attribute(attr).reindex(all_nodes)\n",
    "\n",
    "    elev_s      = q_node('elevation')\n",
    "    coords_s    = q_node('coordinates')\n",
    "    base_dem_s  = q_node('base_demand')\n",
    "    node_type_s = pd.Series({n: wn.get_node(n).node_type for n in all_nodes},\n",
    "                            index=all_nodes, name=\"node_type\")\n",
    "\n",
    "    x_s = coords_s.apply(lambda v: v[0] if isinstance(v, tuple) and len(v)==2 else np.nan)\n",
    "    y_s = coords_s.apply(lambda v: v[1] if isinstance(v, tuple) and len(v)==2 else np.nan)\n",
    "\n",
    "    nodes_df = pd.DataFrame({\n",
    "        'node_type': node_type_s,\n",
    "        'elevation_m': elev_s,\n",
    "        'base_demand_m3s': base_dem_s,\n",
    "        'x': x_s,\n",
    "        'y': y_s\n",
    "    }, index=all_nodes)\n",
    "\n",
    "    length_s = wn.query_link_attribute('length')\n",
    "    diam_s   = wn.query_link_attribute('diameter')\n",
    "    rough_s  = wn.query_link_attribute('roughness')\n",
    "    status_s = wn.query_link_attribute('status')\n",
    "\n",
    "    rows = []\n",
    "    for lname in wn.link_name_list:\n",
    "        L = wn.get_link(lname)\n",
    "        rows.append({\n",
    "            \"link_name\": lname,\n",
    "            \"link_type\": L.link_type,                # Pipe/Pump/Valve\n",
    "            \"from_node\": L.start_node_name,\n",
    "            \"to_node\":   L.end_node_name,\n",
    "            \"length_m\":  _safe_float(length_s.get(lname), np.nan),\n",
    "            \"diameter_m\":_safe_float(diam_s.get(lname),   np.nan),\n",
    "            \"roughness\": _safe_float(rough_s.get(lname),  np.nan),\n",
    "            \"status\":    status_s.get(lname)\n",
    "        })\n",
    "    links_df = pd.DataFrame(rows).set_index(\"link_name\")\n",
    "\n",
    "    # Costruisci grafo\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(all_nodes)\n",
    "    for r in links_df.reset_index().itertuples(index=False):\n",
    "        w = _safe_float(r.length_m, 1.0) if use_length_as_weight else 1.0\n",
    "        G.add_edge(r.from_node, r.to_node,\n",
    "                   link_name=r.link_name,\n",
    "                   link_type=r.link_type,\n",
    "                   length_m=_safe_float(r.length_m, None),\n",
    "                   diameter_m=_safe_float(r.diameter_m, None),\n",
    "                   roughness=_safe_float(r.roughness, None),\n",
    "                   status=r.status,\n",
    "                   weight=w)\n",
    "    return nodes_df, links_df, G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31eee1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# graph_metrics.py\n",
    "from typing import Dict, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def compute_node_edge_metrics(\n",
    "    nodes_df: pd.DataFrame,\n",
    "    links_df: pd.DataFrame,\n",
    "    G: nx.Graph,\n",
    "    use_length_as_weight: bool = True\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Aggiunge metriche di NODO e di ARCO e calcola metriche GLOBALI.\n",
    "    Ritorna: nodes_df_ext, links_df_ext, globals_dict\n",
    "    \"\"\"\n",
    "    all_nodes = nodes_df.index\n",
    "\n",
    "    weight_kw = {'weight': 'weight'} if use_length_as_weight else {}\n",
    "    # Nodi\n",
    "    deg_s      = pd.Series(dict(G.degree()), name='degree').reindex(all_nodes).fillna(0).astype(int)\n",
    "    deg_cent_s = pd.Series(nx.degree_centrality(G), name='degree_centrality').reindex(all_nodes)\n",
    "    btw_s      = pd.Series(nx.betweenness_centrality(G, normalized=True, **weight_kw),\n",
    "                           name='betweenness_centrality').reindex(all_nodes)\n",
    "    clo_s      = pd.Series(nx.closeness_centrality(G, distance=weight_kw.get('weight')),\n",
    "                           name='closeness_centrality').reindex(all_nodes)\n",
    "    clu_s      = pd.Series(nx.clustering(G), name='clustering_coeff').reindex(all_nodes)\n",
    "    try:\n",
    "        core_s = pd.Series(nx.core_number(G), name='core_number').reindex(all_nodes)\n",
    "    except Exception:\n",
    "        core_s = pd.Series(index=all_nodes, dtype=float, name='core_number')\n",
    "\n",
    "    bridges = set(nx.bridges(G)) if G.number_of_edges() > 0 else set()\n",
    "    bridge_count = {n: 0 for n in all_nodes}\n",
    "    for u, v in bridges:\n",
    "        bridge_count[u] += 1\n",
    "        bridge_count[v] += 1\n",
    "    bridge_cnt_s = pd.Series(bridge_count, name='bridge_count').reindex(all_nodes).fillna(0).astype(int)\n",
    "    bridge_ratio = (bridge_cnt_s / deg_s.replace(0, np.nan)).rename('bridge_ratio')\n",
    "\n",
    "    nodes_ext = pd.concat([nodes_df, deg_s, deg_cent_s, btw_s, clo_s, clu_s, core_s, bridge_cnt_s, bridge_ratio], axis=1)\n",
    "\n",
    "    # Archi\n",
    "    ebw = nx.edge_betweenness_centrality(G, normalized=True, **weight_kw) if G.number_of_edges() > 0 else {}\n",
    "    edge_rows = []\n",
    "    for r in links_df.reset_index().itertuples(index=False):\n",
    "        uv = (r.from_node, r.to_node)\n",
    "        eb = ebw.get(uv, ebw.get((uv[1], uv[0]), 0.0))\n",
    "        is_bridge = (uv in bridges) or ((uv[1], uv[0]) in bridges)\n",
    "        edge_rows.append({\"link_name\": r.link_name, \"edge_betweenness\": eb, \"is_bridge\": float(is_bridge)})\n",
    "    edges_metrics_df = pd.DataFrame(edge_rows).set_index(\"link_name\")\n",
    "    links_ext = links_df.join(edges_metrics_df, how='left')\n",
    "\n",
    "    # Globali\n",
    "    globals_out = {}\n",
    "    try:\n",
    "        globals_out['assortativity'] = nx.degree_assortativity_coefficient(G)\n",
    "    except Exception:\n",
    "        globals_out['assortativity'] = None\n",
    "\n",
    "    if nx.is_empty(G) or G.number_of_edges() == 0:\n",
    "        C = L = None\n",
    "    else:\n",
    "        GC_nodes = max(nx.connected_components(G), key=len)\n",
    "        GC = G.subgraph(GC_nodes).copy()\n",
    "        C = nx.average_clustering(GC)\n",
    "        try:\n",
    "            L = nx.average_shortest_path_length(GC, weight=weight_kw.get('weight'))\n",
    "        except Exception:\n",
    "            L = None\n",
    "    globals_out['avg_clustering_C'] = C\n",
    "    globals_out['avg_path_length_L'] = L\n",
    "\n",
    "    try:\n",
    "        from networkx.algorithms.smallworld import sigma as nx_sigma, omega as nx_omega\n",
    "        globals_out['small_world_sigma'] = nx_sigma(G, niter=10, nrand=10, seed=42)\n",
    "        globals_out['small_world_omega'] = nx_omega(G, niter=10, nrand=10, seed=42)\n",
    "        globals_out['C_rand'] = None\n",
    "        globals_out['L_rand'] = None\n",
    "    except Exception:\n",
    "        globals_out.update({'C_rand': None, 'L_rand': None, 'small_world_sigma': None, 'small_world_omega': None})\n",
    "\n",
    "    return nodes_ext, links_ext, globals_out\n",
    "\n",
    "def save_globals(path_json: str, globals_dict: Dict):\n",
    "    with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(globals_dict, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863c2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# build_pyg_data.py\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "STATUS_OPEN_VALUES = {\"Open\", 1, \"OPEN\", \"open\", True}\n",
    "\n",
    "def one_hot(series: pd.Series, categories: List[str], prefix: str) -> pd.DataFrame:\n",
    "    out = {}\n",
    "    for cat in categories:\n",
    "        out[f\"{prefix}_{cat}\"] = (series == cat).astype(float)\n",
    "    return pd.DataFrame(out, index=series.index)\n",
    "\n",
    "def zscore(df: pd.DataFrame, cols: List[str]):\n",
    "    stats = {}\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        m = df[c].astype(float).mean(skipna=True)\n",
    "        s = df[c].astype(float).std(skipna=True)\n",
    "        s = float(s) if s and s > 0 else 1.0\n",
    "        stats[c] = {\"mean\": float(m), \"std\": float(s)}\n",
    "        df[c] = (df[c].astype(float) - m) / s\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "    return df, stats\n",
    "\n",
    "def build_pyg_tensors(\n",
    "    nodes_df: pd.DataFrame,\n",
    "    links_df: pd.DataFrame,\n",
    "    node_type_cats: List[str] = None,\n",
    "    link_type_cats: List[str] = None,\n",
    ") -> Tuple[Data, Dict]:\n",
    "    \"\"\"\n",
    "    Converte nodes_df + links_df in PyG Data (x, edge_index, edge_attr) + metadata (mapping, scaler, ordini feature).\n",
    "    \"\"\"\n",
    "    if node_type_cats is None:\n",
    "        node_type_cats = [\"Junction\", \"Reservoir\", \"Tank\"]\n",
    "    if link_type_cats is None:\n",
    "        link_type_cats = [\"Pipe\", \"Pump\", \"Valve\"]\n",
    "\n",
    "    # One-hot\n",
    "    node_type_oh = one_hot(nodes_df[\"node_type\"], node_type_cats, prefix=\"nodeType\")\n",
    "    link_type_oh = one_hot(links_df[\"link_type\"], link_type_cats, prefix=\"linkType\")\n",
    "    status_oh = pd.DataFrame({\n",
    "        \"status_open\": links_df[\"status\"].apply(lambda s: 1.0 if s in STATUS_OPEN_VALUES else 0.0),\n",
    "    }, index=links_df.index)\n",
    "\n",
    "    # Numeriche\n",
    "    node_num_cols = [\n",
    "        \"elevation_m\", \"base_demand_m3s\", \"x\", \"y\",\n",
    "        \"degree\", \"degree_centrality\", \"betweenness_centrality\",\n",
    "        \"closeness_centrality\", \"clustering_coeff\", \"core_number\",\n",
    "        \"bridge_count\", \"bridge_ratio\"\n",
    "    ]\n",
    "    edge_num_cols = [\n",
    "        \"length_m\", \"diameter_m\", \"roughness\", \"edge_betweenness\", \"is_bridge\"\n",
    "    ]\n",
    "\n",
    "    nodes_num_norm, node_stats = zscore(nodes_df, node_num_cols)\n",
    "    edges_num_norm, edge_stats = zscore(links_df, edge_num_cols)\n",
    "\n",
    "    X_nodes = pd.concat([nodes_num_norm[node_num_cols], node_type_oh], axis=1).fillna(0.0)\n",
    "    X_edges = pd.concat([edges_num_norm[edge_num_cols], link_type_oh, status_oh], axis=1).fillna(0.0)\n",
    "\n",
    "    # Mapping\n",
    "    all_nodes = list(nodes_df.index)\n",
    "    node2idx = {n: i for i, n in enumerate(all_nodes)}\n",
    "    idx2node = {i: n for n, i in node2idx.items()}\n",
    "\n",
    "    # edge_index (non orientato -> due direzioni)\n",
    "    edge_index_list = []\n",
    "    for r in links_df.reset_index().itertuples(index=False):\n",
    "        u = node2idx[r.from_node]\n",
    "        v = node2idx[r.to_node]\n",
    "        edge_index_list.append([u, v])\n",
    "        edge_index_list.append([v, u])\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).T  # [2, 2E]\n",
    "\n",
    "    # edge_attr (duplicato)\n",
    "    edge_attr_src = torch.tensor(X_edges.values, dtype=torch.float32)  # [E, Fe]\n",
    "    edge_attr = torch.vstack([edge_attr_src, edge_attr_src])          # [2E, Fe]\n",
    "\n",
    "    x = torch.tensor(X_nodes.values, dtype=torch.float32)             # [N, Fv]\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        num_nodes=x.shape[0]\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"node2idx\": node2idx,\n",
    "        \"idx2node\": idx2node,\n",
    "        \"node_feat_order\": list(X_nodes.columns),\n",
    "        \"edge_feat_order\": list(X_edges.columns),\n",
    "        \"node_type_cats\": node_type_cats,\n",
    "        \"link_type_cats\": link_type_cats,\n",
    "        \"scalers\": {\"node_stats\": node_stats, \"edge_stats\": edge_stats}\n",
    "    }\n",
    "    return data, meta\n",
    "\n",
    "def save_artifacts(out_dir, data: Data, meta: Dict, globals_dict: Dict = None):\n",
    "    import torch, json\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(data, out_dir / \"graph_data.pt\")\n",
    "    with open(out_dir / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    if globals_dict is not None:\n",
    "        with open(out_dir / \"graph_globals.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(globals_dict, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e909812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# models_dqn.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN encoder che restituisce embeddings di nodo [N,H] e embedding globale [B,H] (mean pooling).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden: int = 128, out_channels: int = 128, num_layers: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 1\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden, hidden))\n",
    "        if num_layers > 1:\n",
    "            self.convs.append(GCNConv(hidden, out_channels))\n",
    "            self.out_dim = out_channels\n",
    "        else:\n",
    "            self.out_dim = hidden\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = self.act(x)\n",
    "                x = self.dropout(x)\n",
    "        node_embeddings = x\n",
    "        if hasattr(data, \"batch\") and data.batch is not None:\n",
    "            g = global_mean_pool(node_embeddings, data.batch)\n",
    "        else:\n",
    "            g = node_embeddings.mean(dim=0, keepdim=True)\n",
    "        return node_embeddings, g\n",
    "\n",
    "class NodeActionDQNHead(nn.Module):\n",
    "    \"\"\"Q per azioni per-nodo (output [N, A]).\"\"\"\n",
    "    def __init__(self, emb_dim: int, action_dim_per_node: int = 1):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, action_dim_per_node)\n",
    "        )\n",
    "    def forward(self, node_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ff(node_embeddings)\n",
    "\n",
    "class GlobalActionDQNHead(nn.Module):\n",
    "    \"\"\"Q per azioni globali (output [B, n_actions]).\"\"\"\n",
    "    def __init__(self, emb_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, n_actions)\n",
    "        )\n",
    "    def forward(self, graph_emb: torch.Tensor) -> torch.Tensor:\n",
    "        return self.q(graph_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f252be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# action_space.py\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Un \"open\" generoso per gli status che arrivano da WNTR\n",
    "STATUS_OPEN_VALUES = {\"Open\", 1, \"OPEN\", \"open\", True}\n",
    "\n",
    "def _is_open(x) -> bool:\n",
    "    return x in STATUS_OPEN_VALUES\n",
    "\n",
    "def _adjacent_links(links_df: pd.DataFrame, node: str) -> pd.DataFrame:\n",
    "    df = links_df.reset_index()\n",
    "    return df[(df[\"from_node\"] == node) | (df[\"to_node\"] == node)]\n",
    "\n",
    "# --------------------------\n",
    "#  NODE-LEVEL ACTION SPACE\n",
    "# --------------------------\n",
    "def build_node_action_space(nodes_df: pd.DataFrame,\n",
    "                            links_df: pd.DataFrame\n",
    "                            ) -> Tuple[List[str], np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Ritorna:\n",
    "      - node_actions: elenco nome azioni per-nodo [A_n]\n",
    "      - node_action_mask: matrice [N, A_n] (bool) con True dove l'azione è valida per quel nodo\n",
    "      - info: dizionario con appigli utili (per debug/ambiente)\n",
    "    Azioni definite SOLO in base a feature/metriche/typing già presenti.\n",
    "    \"\"\"\n",
    "    # Azioni per nodo (5):\n",
    "    #  0) noop                         -> sempre possibile\n",
    "    #  1) open_adjacent_valve         -> valida se c'è almeno 1 valvola adiacente chiusa\n",
    "    #  2) close_adjacent_valve        -> valida se c'è almeno 1 valvola adiacente aperta\n",
    "    #  3) pump_up_adjacent            -> valida se c'è almeno 1 pompa adiacente (aperta)\n",
    "    #  4) pump_down_adjacent          -> valida se c'è almeno 1 pompa adiacente (aperta)\n",
    "    node_actions = [\n",
    "        \"noop\",\n",
    "        \"open_adjacent_valve\",\n",
    "        \"close_adjacent_valve\",\n",
    "        \"pump_up_adjacent\",\n",
    "        \"pump_down_adjacent\",\n",
    "    ]\n",
    "\n",
    "    N = len(nodes_df.index)\n",
    "    A = len(node_actions)\n",
    "    mask = np.zeros((N, A), dtype=bool)\n",
    "\n",
    "    # Pre-calcolo per velocità\n",
    "    links = links_df.reset_index()\n",
    "    # Map node -> adjacent rows\n",
    "    adj_map: Dict[str, pd.DataFrame] = {}\n",
    "    for n in nodes_df.index:\n",
    "        adj_map[n] = links[(links[\"from_node\"] == n) | (links[\"to_node\"] == n)]\n",
    "\n",
    "    # Costruisci la maschera\n",
    "    for i, n in enumerate(nodes_df.index):\n",
    "        adj = adj_map[n]\n",
    "        # 0) noop\n",
    "        mask[i, 0] = True\n",
    "\n",
    "        # Valvole adiacenti\n",
    "        adj_valves = adj[adj[\"link_type\"] == \"Valve\"]\n",
    "        any_valve_closed = (len(adj_valves) > 0) and any(not _is_open(s) for s in adj_valves[\"status\"].tolist())\n",
    "        any_valve_open   = (len(adj_valves) > 0) and any(_is_open(s)  for s in adj_valves[\"status\"].tolist())\n",
    "\n",
    "        # Pompe adiacenti (consideriamo \"valida\" se sono presenti; opz: richiedere che siano aperte)\n",
    "        adj_pumps = adj[adj[\"link_type\"] == \"Pump\"]\n",
    "        any_pump_open = (len(adj_pumps) > 0) and any(_is_open(s) for s in adj_pumps[\"status\"].tolist())\n",
    "\n",
    "        # 1) open_adjacent_valve\n",
    "        mask[i, 1] = any_valve_closed\n",
    "        # 2) close_adjacent_valve\n",
    "        mask[i, 2] = any_valve_open\n",
    "        # 3) pump_up_adjacent\n",
    "        mask[i, 3] = any_pump_open\n",
    "        # 4) pump_down_adjacent\n",
    "        mask[i, 4] = any_pump_open\n",
    "\n",
    "    info = {\n",
    "        \"node_index_order\": list(nodes_df.index),\n",
    "        \"node_actions\": node_actions,\n",
    "        \"note\": \"Le azioni per-nodo sono abilitate solo se esiste almeno un link adiacente del tipo richiesto (valvola/pompa) con status compatibile.\"\n",
    "    }\n",
    "    return node_actions, mask, info\n",
    "\n",
    "# --------------------------\n",
    "#  GLOBAL ACTION SPACE\n",
    "# --------------------------\n",
    "def build_global_action_space(nodes_df: pd.DataFrame,\n",
    "                              links_df: pd.DataFrame,\n",
    "                              low_pct: float = 0.2,\n",
    "                              high_pct: float = 0.8\n",
    "                              ) -> Tuple[List[str], np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Definisce un set finito di azioni globali guidate da metriche:\n",
    "      0) noop\n",
    "      1) open_valves_on_low_edge_betweenness   (ridondanza: apri valvole su archi poco \"centrali\")\n",
    "      2) close_valves_on_high_edge_betweenness (contenimento: chiudi valvole su archi molto \"centrali\")\n",
    "      3) boost_pumps_near_low_closeness_nodes  (aiuta i nodi periferici)\n",
    "      4) throttle_all_pumps                    (risparmio energetico)\n",
    "      5) open_valves_on_bridges                (favorisci accessi su archi-ponti)\n",
    "    Ritorna:\n",
    "      - global_actions: lista nomi azioni [A_g]\n",
    "      - global_mask: vettore [A_g] (bool) se l'azione ha senso dato lo stato/feature\n",
    "      - info: dizionario con target suggeriti (indici link/nodi) per ogni azione\n",
    "    \"\"\"\n",
    "    actions = [\n",
    "        \"noop\",\n",
    "        \"open_valves_on_low_edge_betweenness\",\n",
    "        \"close_valves_on_high_edge_betweenness\",\n",
    "        \"boost_pumps_near_low_closeness_nodes\",\n",
    "        \"throttle_all_pumps\",\n",
    "        \"open_valves_on_bridges\",\n",
    "    ]\n",
    "    A = len(actions)\n",
    "    mask = np.zeros((A,), dtype=bool)\n",
    "    info: Dict[str, Dict] = {}\n",
    "\n",
    "    L = links_df.reset_index()\n",
    "    N = nodes_df.reset_index().rename(columns={\"index\": \"node_name\"})\n",
    "\n",
    "    # Edge betweenness percentili\n",
    "    if \"edge_betweenness\" in links_df.columns and links_df[\"edge_betweenness\"].notna().any():\n",
    "        eb = links_df[\"edge_betweenness\"].fillna(0.0).values\n",
    "        low_thr  = np.quantile(eb, low_pct)\n",
    "        high_thr = np.quantile(eb, high_pct)\n",
    "    else:\n",
    "        low_thr, high_thr = 0.0, 1.0\n",
    "\n",
    "    # 0) noop\n",
    "    mask[0] = True\n",
    "    info[\"noop\"] = {}\n",
    "\n",
    "    # 1) open_valves_on_low_edge_betweenness\n",
    "    valves = L[L[\"link_type\"] == \"Valve\"].copy()\n",
    "    low_set = valves[valves[\"edge_betweenness\"].fillna(0.0) <= low_thr]\n",
    "    mask[1] = len(low_set) > 0\n",
    "    info[actions[1]] = {\n",
    "        \"target_link_names\": low_set[\"link_name\"].tolist(),\n",
    "        \"description\": \"Apri le valvole sui rami con bassa edge-betweenness per aumentare la ridondanza locale.\"\n",
    "    }\n",
    "\n",
    "    # 2) close_valves_on_high_edge_betweenness\n",
    "    high_set = valves[valves[\"edge_betweenness\"].fillna(0.0) >= high_thr]\n",
    "    mask[2] = len(high_set) > 0\n",
    "    info[actions[2]] = {\n",
    "        \"target_link_names\": high_set[\"link_name\"].tolist(),\n",
    "        \"description\": \"Chiudi le valvole sui colli di bottiglia (alta edge-betweenness) per contenere/isolarsi.\"\n",
    "    }\n",
    "\n",
    "    # 3) boost_pumps_near_low_closeness_nodes\n",
    "    #    Trova nodi con closeness bassa (periferici) e pompa adiacente\n",
    "    if \"closeness_centrality\" in nodes_df.columns and nodes_df[\"closeness_centrality\"].notna().any():\n",
    "        clo = nodes_df[\"closeness_centrality\"].fillna(0.0).values\n",
    "        low_clo_thr = np.quantile(clo, low_pct)\n",
    "        low_nodes = set(nodes_df[nodes_df[\"closeness_centrality\"].fillna(0.0) <= low_clo_thr].index)\n",
    "    else:\n",
    "        low_nodes = set()\n",
    "    pumps = L[L[\"link_type\"] == \"Pump\"].copy()\n",
    "    pump_targets = pumps[(pumps[\"from_node\"].isin(low_nodes)) | (pumps[\"to_node\"].isin(low_nodes))]\n",
    "    mask[3] = len(pump_targets) > 0\n",
    "    info[actions[3]] = {\n",
    "        \"target_link_names\": pump_targets[\"link_name\"].tolist(),\n",
    "        \"description\": \"Aumenta la spinta delle pompe prossime ai nodi più periferici (closeness bassa).\"\n",
    "    }\n",
    "\n",
    "    # 4) throttle_all_pumps\n",
    "    mask[4] = len(pumps) > 0\n",
    "    info[actions[4]] = {\n",
    "        \"target_link_names\": pumps[\"link_name\"].tolist(),\n",
    "        \"description\": \"Riduci la spinta di tutte le pompe per risparmio energetico.\"\n",
    "    }\n",
    "\n",
    "    # 5) open_valves_on_bridges\n",
    "    bridge_valves = valves[valves.get(\"is_bridge\", 0.0) > 0.5]\n",
    "    mask[5] = len(bridge_valves) > 0\n",
    "    info[actions[5]] = {\n",
    "        \"target_link_names\": bridge_valves[\"link_name\"].tolist(),\n",
    "        \"description\": \"Apri valvole su archi che sono ponti strutturali, per garantire connettività.\"\n",
    "    }\n",
    "\n",
    "    return actions, mask, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878eed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> node_emb: (97, 128) g_emb: (1, 128)\n",
      "Q-node: (97, 5) Q-global: (1, 6)\n",
      "Node actions: ['noop', 'open_adjacent_valve', 'close_adjacent_valve', 'pump_up_adjacent', 'pump_down_adjacent']\n",
      "Global actions: ['noop', 'open_valves_on_low_edge_betweenness', 'close_valves_on_high_edge_betweenness', 'boost_pumps_near_low_closeness_nodes', 'throttle_all_pumps', 'open_valves_on_bridges']\n",
      "Node-action mask shape: (97, 5)  Global-action mask: [True, False, False, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "# main_example.py (solo le parti che cambiano/aggiungi)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "INP_PATH = r\"C:\\Users\\nephr\\Desktop\\Uni Nuova\\Tesi\\WNTR-main\\WNTR-main\\examples\\networks\\Net3.inp\"\n",
    "\n",
    "# >>> salva nella directory del notebook <<<\n",
    "OUT_DIR = Path.cwd()\n",
    "USE_LENGTH_AS_WEIGHT = True\n",
    "\n",
    "def main():\n",
    "    # 1) Import + feature base\n",
    "    nodes_df, links_df, G = load_wdn(INP_PATH, use_length_as_weight=USE_LENGTH_AS_WEIGHT)\n",
    "\n",
    "    # 2) Metriche (nodi/archi + globali)\n",
    "    nodes_ext, links_ext, globals_dict = compute_node_edge_metrics(\n",
    "        nodes_df, links_df, G, use_length_as_weight=USE_LENGTH_AS_WEIGHT\n",
    "    )\n",
    "    save_globals(OUT_DIR / \"globals.json\", globals_dict)\n",
    "\n",
    "    # 3) Costruzione tensori PyG (+ salvataggi)\n",
    "    data, meta = build_pyg_tensors(nodes_ext, links_ext)\n",
    "    save_artifacts(OUT_DIR, data, meta, globals_dict)\n",
    "\n",
    "    # 4) Costruisci spazi azioni + maschere\n",
    "    node_actions, node_mask, node_info = build_node_action_space(nodes_ext, links_ext)\n",
    "    global_actions, global_mask, global_info = build_global_action_space(nodes_ext, links_ext)\n",
    "\n",
    "    # 5) Encoder + DQN heads con le giuste dimensioni\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data.batch = torch.zeros(data.num_nodes, dtype=torch.long)  # batch singolo\n",
    "\n",
    "    encoder = GNNEncoder(in_channels=data.x.shape[1], hidden=128, out_channels=128, num_layers=3).to(device)\n",
    "    # per-nodo: dimensione = n° azioni per-nodo\n",
    "    node_head = NodeActionDQNHead(emb_dim=encoder.out_dim, action_dim_per_node=len(node_actions)).to(device)\n",
    "    # globale: dimensione = n° azioni globali\n",
    "    global_head = GlobalActionDQNHead(emb_dim=encoder.out_dim, n_actions=len(global_actions)).to(device)\n",
    "\n",
    "    data = data.to(device)\n",
    "    node_emb, g_emb = encoder(data)\n",
    "    q_node = node_head(node_emb)        # [N, A_node]\n",
    "    q_global = global_head(g_emb)       # [1, A_global]\n",
    "\n",
    "    print(\"Shapes -> node_emb:\", tuple(node_emb.shape), \"g_emb:\", tuple(g_emb.shape))\n",
    "    print(\"Q-node:\", tuple(q_node.shape), \"Q-global:\", tuple(q_global.shape))\n",
    "    print(\"Node actions:\", node_actions)\n",
    "    print(\"Global actions:\", global_actions)\n",
    "    print(\"Node-action mask shape:\", node_mask.shape, \" Global-action mask:\", global_mask.tolist())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3d280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 0] NODE[60] -> pump_down_adjacent | applied=True targets=['335']\n",
      "           reward = -0.1500  (struct=0.0000  cost=0.1500)\n",
      "[Step 1] NODE[60] -> pump_up_adjacent | applied=True targets=['335']\n",
      "           reward = -0.1500  (struct=0.0000  cost=0.1500)\n",
      "[Step 2] GLOBAL -> noop | applied=False targets=[]\n",
      "           reward = 0.0000  (struct=0.0000  cost=0.0000)\n",
      "\n",
      "Log salvato in: run3_log.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Esegue 3 step consecutivi:\n",
    "  stato -> GNN -> DQN -> azione -> modifica WNTR -> ricalcolo feature/metriche -> nuovo stato\n",
    "Salva/legge tutto nella directory corrente.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import wntr\n",
    "\n",
    "# === CONFIG ===\n",
    "INP_PATH = r\"C:\\Users\\nephr\\Desktop\\Uni Nuova\\Tesi\\WNTR-main\\WNTR-main\\examples\\networks\\Net3.inp\"\n",
    "EPSILON = 0.20          # ε-greedy\n",
    "HIDDEN  = 128           # dim embedding\n",
    "STEPS   = 3             # numero di step da eseguire\n",
    "USE_LENGTH_AS_WEIGHT = True  # metriche pesate su 'length'\n",
    "\n",
    "# === Helper: scelta azioni con maschere ===\n",
    "def select_node_action(q_node: torch.Tensor, node_mask, epsilon: float = 0.1):\n",
    "    mask_t = torch.as_tensor(node_mask, dtype=torch.bool, device=q_node.device)\n",
    "    q_masked = q_node.clone()\n",
    "    q_masked[~mask_t] = -1e9\n",
    "    if torch.all(q_masked == -1e9):\n",
    "        return None, None, None  # nessuna azione valida\n",
    "    N, A = q_masked.shape\n",
    "    if torch.rand(()) < epsilon:\n",
    "        valid_positions = torch.nonzero(mask_t, as_tuple=False)\n",
    "        idx = torch.randint(0, valid_positions.shape[0], (1,))\n",
    "        i, a = valid_positions[idx].squeeze(0).tolist()\n",
    "    else:\n",
    "        flat_idx = torch.argmax(q_masked).item()\n",
    "        i = flat_idx // A\n",
    "        a = flat_idx % A\n",
    "    return int(i), int(a), float(q_masked[i, a].item())\n",
    "\n",
    "def select_global_action(q_global: torch.Tensor, global_mask, epsilon: float = 0.1):\n",
    "    q = q_global.squeeze(0) if q_global.ndim == 2 else q_global\n",
    "    mask_t = torch.as_tensor(global_mask, dtype=torch.bool, device=q.device)\n",
    "    q_masked = q.clone()\n",
    "    q_masked[~mask_t] = -1e9\n",
    "    if torch.all(q_masked == -1e9):\n",
    "        return None, None\n",
    "    if torch.rand(()) < epsilon:\n",
    "        valid_actions = torch.nonzero(mask_t, as_tuple=False).squeeze(1)\n",
    "        a = valid_actions[torch.randint(0, valid_actions.numel(), (1,))].item()\n",
    "    else:\n",
    "        a = torch.argmax(q_masked).item()\n",
    "    return int(a), float(q_masked[a].item())\n",
    "\n",
    "# === Helper: applicazione azioni su WNTR ===\n",
    "\n",
    "def _norm_status(s) -> str:\n",
    "    \"\"\"\n",
    "    Converte lo status in 'open' / 'closed' (o 'unknown').\n",
    "    Gestisce stringhe, bool, int, float, e Enum WNTR con .name.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"unknown\"\n",
    "    # Enum di WNTR: .name -> \"Open\" / \"Closed\"\n",
    "    if hasattr(s, \"name\"):\n",
    "        try:\n",
    "            return str(s.name).strip().lower()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # stringhe\n",
    "    if isinstance(s, str):\n",
    "        t = s.strip().lower()\n",
    "        if t in (\"open\", \"opened\", \"on\", \"true\", \"1\"):\n",
    "            return \"open\"\n",
    "        if t in (\"closed\", \"close\", \"off\", \"false\", \"0\"):\n",
    "            return \"closed\"\n",
    "        return t\n",
    "    # booleani\n",
    "    if isinstance(s, bool):\n",
    "        return \"open\" if s else \"closed\"\n",
    "    # numeri\n",
    "    if isinstance(s, (int, float)):\n",
    "        try:\n",
    "            return \"open\" if int(s) != 0 else \"closed\"\n",
    "        except Exception:\n",
    "            return \"unknown\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def _is_open(s) -> bool:\n",
    "    return _norm_status(s) == \"open\"\n",
    "\n",
    "def _is_closed(s) -> bool:\n",
    "    return _norm_status(s) == \"closed\"\n",
    "def _pick_adjacent(links_df, node_name, kind=\"Valve\", need_open=None, need_closed=None):\n",
    "    df = links_df.reset_index()\n",
    "    adj = df[(df[\"from_node\"] == node_name) | (df[\"to_node\"] == node_name)]\n",
    "    sub = adj[adj[\"link_type\"] == kind].copy()\n",
    "    if need_open is True:\n",
    "        sub = sub[sub[\"status\"].apply(_is_open)]\n",
    "    if need_closed is True:\n",
    "        sub = sub[sub[\"status\"].apply(_is_closed)]\n",
    "    if len(sub) == 0:\n",
    "        return None\n",
    "    # criterio: scegli diametro più grande (puoi cambiare)\n",
    "    sub = sub.sort_values(\"diameter_m\", ascending=False)\n",
    "    return sub.iloc[0][\"link_name\"]\n",
    "\n",
    "def _get_speed(pump, default=1.0):\n",
    "    try:\n",
    "        return float(getattr(pump, \"speed\", default))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _set_speed(pump, value):\n",
    "    # clamp per sicurezza\n",
    "    try:\n",
    "        pump.speed = max(0.0, float(value))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _pump_boost(pump, delta=0.1, max_speed=3.0):\n",
    "    s = _get_speed(pump, 1.0) + float(delta)\n",
    "    _set_speed(pump, min(s, max_speed))\n",
    "\n",
    "def _pump_throttle(pump, delta=0.1):\n",
    "    s = _get_speed(pump, 1.0) - float(delta)\n",
    "    _set_speed(pump, max(0.0, s))\n",
    "\n",
    "def _pump_open_min(pump, min_speed=0.1):\n",
    "    # emula \"open\": garantisci una speed > 0\n",
    "    s = _get_speed(pump, 0.0)\n",
    "    if s <= 0.0:\n",
    "        _set_speed(pump, min_speed)\n",
    "\n",
    "def _pump_close(pump):\n",
    "    # emula \"closed\": speed = 0.0\n",
    "    _set_speed(pump, 0.0)\n",
    "\n",
    "\n",
    "def apply_node_action_to_wn(wn, nodes_df, links_df, node_idx, action_name):\n",
    "    node_name = nodes_df.index[node_idx]\n",
    "    if action_name == \"noop\":\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "    elif action_name == \"open_adjacent_valve\":\n",
    "        link_name = _pick_adjacent(links_df, node_name, kind=\"Valve\", need_closed=True)\n",
    "        if link_name:\n",
    "            wn.get_link(link_name).status = wntr.network.base.LinkStatus.Open\n",
    "            return {\"applied\": True, \"targets\": [link_name]}\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "    elif action_name == \"close_adjacent_valve\":\n",
    "        link_name = _pick_adjacent(links_df, node_name, kind=\"Valve\", need_open=True)\n",
    "        if link_name:\n",
    "            wn.get_link(link_name).status = wntr.network.base.LinkStatus.Closed\n",
    "            return {\"applied\": True, \"targets\": [link_name]}\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "    elif action_name == \"pump_up_adjacent\":\n",
    "        link_name = _pick_adjacent(links_df, node_name, kind=\"Pump\")\n",
    "        if link_name:\n",
    "            pump = wn.get_link(link_name)\n",
    "            _pump_open_min(pump, min_speed=0.1)  # assicurati che sia \"aperta\" (speed > 0)\n",
    "            _pump_boost(pump, delta=0.1)         # aumenta la speed\n",
    "            return {\"applied\": True, \"targets\": [link_name]}\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "\n",
    "    elif action_name == \"pump_down_adjacent\":\n",
    "        link_name = _pick_adjacent(links_df, node_name, kind=\"Pump\")\n",
    "        if link_name:\n",
    "            pump = wn.get_link(link_name)\n",
    "            _pump_throttle(pump, delta=0.1)      # riduci la speed (fino a 0 = \"chiusa\")\n",
    "            return {\"applied\": True, \"targets\": [link_name]}\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "    else:\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "\n",
    "def apply_global_action_to_wn(wn, links_df, action_name, action_info):\n",
    "    targets = action_info.get(action_name, {}).get(\"target_link_names\", [])\n",
    "    applied = False\n",
    "    if action_name == \"noop\":\n",
    "        return {\"applied\": False, \"targets\": []}\n",
    "    if action_name in (\"open_valves_on_low_edge_betweenness\",\"open_valves_on_bridges\"):\n",
    "        for ln in targets:\n",
    "            try:\n",
    "                if wn.get_link(ln).link_type == \"Valve\":\n",
    "                    wn.get_link(ln).status = wntr.network.base.LinkStatus.Open\n",
    "                    applied = True\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif action_name == \"close_valves_on_high_edge_betweenness\":\n",
    "        for ln in targets:\n",
    "            try:\n",
    "                if wn.get_link(ln).link_type == \"Valve\":\n",
    "                    wn.get_link(ln).status = wntr.network.base.LinkStatus.Closed\n",
    "                    applied = True\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif action_name == \"boost_pumps_near_low_closeness_nodes\":\n",
    "        for ln in targets:\n",
    "            try:\n",
    "                lk = wn.get_link(ln)\n",
    "                if lk.link_type == \"Pump\":\n",
    "                    _pump_open_min(lk, min_speed=0.1)\n",
    "                    _pump_boost(lk, delta=0.1)\n",
    "                    applied = True\n",
    "            except Exception:\n",
    "                pass\n",
    "    elif action_name == \"throttle_all_pumps\":\n",
    "        for ln in wn.link_name_list:\n",
    "            lk = wn.get_link(ln)\n",
    "            if lk.link_type == \"Pump\":\n",
    "                _pump_throttle(lk, delta=0.1)\n",
    "                applied = True\n",
    "    return {\"applied\": applied, \"targets\": targets}\n",
    "\n",
    "# === Reward: delta metriche strutturali ===\n",
    "def compute_structural_reward(prev_globals, curr_globals, links_prev, links_curr):\n",
    "    # estrai metriche (gestisci None)\n",
    "    C_prev = prev_globals.get(\"avg_clustering_C\", None)\n",
    "    L_prev = prev_globals.get(\"avg_path_length_L\", None)\n",
    "    C_curr = curr_globals.get(\"avg_clustering_C\", None)\n",
    "    L_curr = curr_globals.get(\"avg_path_length_L\", None)\n",
    "\n",
    "    def safe_mean_edge_bet(df):\n",
    "        if df is None or \"edge_betweenness\" not in df.columns:\n",
    "            return None\n",
    "        s = df[\"edge_betweenness\"].astype(float)\n",
    "        s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        return float(s.mean()) if len(s) else None\n",
    "\n",
    "    EB_prev = safe_mean_edge_bet(links_prev)\n",
    "    EB_curr = safe_mean_edge_bet(links_curr)\n",
    "\n",
    "    # pesi\n",
    "    wC, wL, wEB = 1.0, 1.0, 0.5\n",
    "\n",
    "    r = 0.0\n",
    "    if C_prev is not None and C_curr is not None:\n",
    "        r += wC * (C_curr - C_prev)        # ↑ meglio\n",
    "    if L_prev is not None and L_curr is not None:\n",
    "        r += wL * (L_prev - C_prev) * 0.0  # (vecchio bug evitato) non usare: lasciamo solo delta L\n",
    "        r += wL * ( (0.0 if L_prev is None else 0.0) )  # placeholder\n",
    "    # correggo: delta L corretto\n",
    "    if L_prev is not None and L_curr is not None:\n",
    "        r += wL * (L_prev - L_curr)        # ↓ meglio\n",
    "    if EB_prev is not None and EB_curr is not None:\n",
    "        r += wEB * (EB_prev - EB_curr)     # ↓ meglio\n",
    "\n",
    "    return float(r)\n",
    "\n",
    "def action_cost(action_kind, action_name, applied, n_targets):\n",
    "    if not applied:\n",
    "        return 0.0\n",
    "    # costi euristici\n",
    "    if action_kind == \"node\":\n",
    "        base = 0.05\n",
    "        if \"valve\" in action_name:\n",
    "            base += 0.05\n",
    "        if \"pump\" in action_name:\n",
    "            base += 0.10\n",
    "        return base\n",
    "    if action_kind == \"global\":\n",
    "        base = 0.10\n",
    "        if action_name == \"throttle_all_pumps\":\n",
    "            base += 0.20\n",
    "        # più target, più costo\n",
    "        return base + 0.01 * max(0, n_targets - 1)\n",
    "    return 0.0\n",
    "\n",
    "# === LOOP 3 STEP ===\n",
    "def run_three_steps():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 0) carica stato iniziale\n",
    "    nodes_df, links_df, G = load_wdn(INP_PATH, use_length_as_weight=USE_LENGTH_AS_WEIGHT)\n",
    "    nodes_ext, links_ext, globals_dict = compute_node_edge_metrics(nodes_df, links_df, G, use_length_as_weight=USE_LENGTH_AS_WEIGHT)\n",
    "\n",
    "    data, meta = build_pyg_tensors(nodes_ext, links_ext)\n",
    "    data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # modelli\n",
    "    encoder = GNNEncoder(in_channels=data.x.shape[1], hidden=HIDDEN, out_channels=HIDDEN, num_layers=3).to(device)\n",
    "    # dimensioni teste determinate dallo spazio azioni allo step corrente\n",
    "    node_actions, node_mask, node_info = build_node_action_space(nodes_ext, links_ext)\n",
    "    global_actions, global_mask, global_info = build_global_action_space(nodes_ext, links_ext)\n",
    "\n",
    "    node_head = NodeActionDQNHead(emb_dim=encoder.out_dim, action_dim_per_node=len(node_actions)).to(device)\n",
    "    global_head = GlobalActionDQNHead(emb_dim=encoder.out_dim, n_actions=len(global_actions)).to(device)\n",
    "\n",
    "    log = []\n",
    "    prev_globals = globals_dict\n",
    "    prev_links   = links_ext.copy()\n",
    "\n",
    "    # usa lo stesso oggetto WNTR per modificarlo nel tempo\n",
    "    wn = wntr.network.WaterNetworkModel(INP_PATH)\n",
    "\n",
    "    for t in range(STEPS):\n",
    "        # 1) GNN forward\n",
    "        node_emb, g_emb = encoder(data)\n",
    "        q_node   = node_head(node_emb)\n",
    "        q_global = global_head(g_emb)\n",
    "\n",
    "        # 2) selezione azione con maschere\n",
    "        node_choice = select_node_action(q_node, node_mask, epsilon=EPSILON)\n",
    "        glob_choice = select_global_action(q_global, global_mask, epsilon=EPSILON)\n",
    "\n",
    "        # confronto dei massimi per scegliere tra (node vs global)\n",
    "        def masked_max(q, mask):\n",
    "            qt = q.clone()\n",
    "            mt = torch.as_tensor(mask, dtype=torch.bool, device=qt.device)\n",
    "            qt[~mt] = -1e9\n",
    "            return float(torch.max(qt).item()) if torch.any(mt) else None\n",
    "\n",
    "        best_node_q = masked_max(q_node, node_mask)\n",
    "        best_glob_q = masked_max(q_global.squeeze(0), global_mask)\n",
    "\n",
    "        if best_node_q is None and best_glob_q is None:\n",
    "            chosen = (\"global\", \"noop\")  # fallback\n",
    "        elif best_glob_q is None or (best_node_q is not None and best_node_q >= best_glob_q):\n",
    "            # usa scelta nodo\n",
    "            ni, ai, qv = node_choice\n",
    "            if ni is None:\n",
    "                chosen = (\"global\", \"noop\")\n",
    "            else:\n",
    "                chosen = (\"node\", (ni, ai, qv))\n",
    "        else:\n",
    "            ai, qv = glob_choice\n",
    "            if ai is None:\n",
    "                chosen = (\"node\", node_choice)\n",
    "            else:\n",
    "                chosen = (\"global\", (ai, qv))\n",
    "\n",
    "        # 3) applicazione azione su WNTR\n",
    "        act_cost = 0.0\n",
    "        applied_info = {\"applied\": False, \"targets\": []}\n",
    "        if chosen[0] == \"node\":\n",
    "            ni, ai, qv = chosen[1]\n",
    "            action_name = node_actions[ai]\n",
    "            applied_info = apply_node_action_to_wn(wn, nodes_ext, links_ext, ni, action_name)\n",
    "            act_cost = action_cost(\"node\", action_name, applied_info[\"applied\"], len(applied_info[\"targets\"]))\n",
    "            action_print = f\"NODE[{nodes_ext.index[ni]}] -> {action_name}\"\n",
    "        else:\n",
    "            ai, qv = chosen[1] if isinstance(chosen[1], tuple) else (0, None)\n",
    "            action_name = global_actions[ai]\n",
    "            applied_info = apply_global_action_to_wn(wn, links_ext, action_name, {**global_info})\n",
    "            act_cost = action_cost(\"global\", action_name, applied_info[\"applied\"], len(applied_info[\"targets\"]))\n",
    "            action_print = f\"GLOBAL -> {action_name}\"\n",
    "\n",
    "        # 4) ricostruisci stato (feature + metriche) dal WNTR MODIFICATO\n",
    "        nodes_df, links_df, G = load_wdn(INP_PATH, use_length_as_weight=USE_LENGTH_AS_WEIGHT)  # ricarica da file? NO!\n",
    "        # ATTENZIONE: sopra ricarica da file; vogliamo invece dal wn modificato:\n",
    "        # quick fix: rigenera nodes/links/G direttamente da wn corrente\n",
    "        # (riutilizziamo loader_features internamente copiandone la logica minima qui)\n",
    "        # Per semplicità: costruiamo da wn (non da file) usando le stesse API WNTR:\n",
    "\n",
    "        # --- ricrea tabelle dal wn modificato ---\n",
    "        all_nodes = wn.node_name_list\n",
    "        import pandas as pd\n",
    "        def q_node_runtime(attr: str) -> pd.Series:\n",
    "            s = wn.query_node_attribute(attr)\n",
    "            return s.reindex(pd.Index(all_nodes, name=\"node_name\"))\n",
    "\n",
    "        elev_s = q_node_runtime('elevation')\n",
    "        coords_s = q_node_runtime('coordinates')\n",
    "        base_dem_s = q_node_runtime('base_demand')\n",
    "        node_type_s = pd.Series({n: wn.get_node(n).node_type for n in all_nodes}, index=pd.Index(all_nodes, name=\"node_name\"), name=\"node_type\")\n",
    "        x_s = coords_s.apply(lambda v: v[0] if isinstance(v, tuple) and len(v)==2 else np.nan)\n",
    "        y_s = coords_s.apply(lambda v: v[1] if isinstance(v, tuple) and len(v)==2 else np.nan)\n",
    "        nodes_now = pd.DataFrame({\n",
    "            \"node_type\": node_type_s, \"elevation_m\": elev_s, \"base_demand_m3s\": base_dem_s, \"x\": x_s, \"y\": y_s\n",
    "        })\n",
    "\n",
    "        length_s = wn.query_link_attribute('length')\n",
    "        diam_s   = wn.query_link_attribute('diameter')\n",
    "        rough_s  = wn.query_link_attribute('roughness')\n",
    "        status_s = wn.query_link_attribute('status')\n",
    "\n",
    "        rows = []\n",
    "        for lname in wn.link_name_list:\n",
    "            L = wn.get_link(lname)\n",
    "            def _sf(v, d=np.nan):\n",
    "                try:\n",
    "                    return float(v)\n",
    "                except Exception:\n",
    "                    return d\n",
    "            rows.append({\n",
    "                \"link_name\": lname,\n",
    "                \"link_type\": L.link_type,\n",
    "                \"from_node\": L.start_node_name,\n",
    "                \"to_node\": L.end_node_name,\n",
    "                \"length_m\": _sf(length_s.get(lname)),\n",
    "                \"diameter_m\": _sf(diam_s.get(lname)),\n",
    "                \"roughness\": _sf(rough_s.get(lname)),\n",
    "                \"status\": status_s.get(lname)\n",
    "            })\n",
    "        links_now = pd.DataFrame(rows).set_index(\"link_name\")\n",
    "\n",
    "        # Grafo aggiornato\n",
    "        import networkx as nx\n",
    "        G_now = nx.Graph()\n",
    "        G_now.add_nodes_from(nodes_now.index)\n",
    "        def _w(v): \n",
    "            try: \n",
    "                return float(v) if v is not None and not np.isnan(v) else 1.0\n",
    "            except Exception:\n",
    "                return 1.0\n",
    "        for r in links_now.reset_index().itertuples(index=False):\n",
    "            G_now.add_edge(r.from_node, r.to_node,\n",
    "                           link_name=r.link_name, link_type=r.link_type,\n",
    "                           length_m=r.length_m, diameter_m=r.diameter_m,\n",
    "                           roughness=r.roughness, status=r.status,\n",
    "                           weight=_w(r.length_m) if USE_LENGTH_AS_WEIGHT else 1.0)\n",
    "\n",
    "        # metriche correnti\n",
    "        nodes_ext_now, links_ext_now, globals_now = compute_node_edge_metrics(\n",
    "            nodes_now, links_now, G_now, use_length_as_weight=USE_LENGTH_AS_WEIGHT\n",
    "        )\n",
    "\n",
    "        # 5) reward = delta metriche - costo azione\n",
    "        r_struct = compute_structural_reward(prev_globals, globals_now, prev_links, links_ext_now)\n",
    "        r = r_struct - act_cost\n",
    "\n",
    "        # 6) rebuild tensori per step successivo\n",
    "        data_next, _ = build_pyg_tensors(nodes_ext_now, links_ext_now)\n",
    "        data_next.batch = torch.zeros(data_next.num_nodes, dtype=torch.long)\n",
    "        data = data_next.to(device)\n",
    "\n",
    "        # ricostruisci spazi azioni e teste (dimensioni possono cambiare)\n",
    "        node_actions, node_mask, node_info = build_node_action_space(nodes_ext_now, links_ext_now)\n",
    "        global_actions, global_mask, global_info = build_global_action_space(nodes_ext_now, links_ext_now)\n",
    "        node_head = NodeActionDQNHead(emb_dim=encoder.out_dim, action_dim_per_node=len(node_actions)).to(device)\n",
    "        global_head = GlobalActionDQNHead(emb_dim=encoder.out_dim, n_actions=len(global_actions)).to(device)\n",
    "\n",
    "        # 7) log e update stato precedente\n",
    "        log.append({\n",
    "            \"step\": t,\n",
    "            \"action_kind\": \"node\" if \"NODE\" in action_print else \"global\",\n",
    "            \"action_str\": action_print,\n",
    "            \"applied\": applied_info[\"applied\"],\n",
    "            \"targets\": applied_info[\"targets\"],\n",
    "            \"reward_structural\": r_struct,\n",
    "            \"action_cost\": act_cost,\n",
    "            \"reward_total\": r,\n",
    "            \"globals_prev\": prev_globals,\n",
    "            \"globals_curr\": globals_now\n",
    "        })\n",
    "\n",
    "        prev_globals = globals_now\n",
    "        prev_links   = links_ext_now.copy()\n",
    "\n",
    "        print(f\"[Step {t}] {action_print} | applied={applied_info['applied']} targets={applied_info['targets']}\")\n",
    "        print(f\"           reward = {r:.4f}  (struct={r_struct:.4f}  cost={act_cost:.4f})\")\n",
    "\n",
    "    # salva log\n",
    "    Path.cwd().joinpath(\"run3_log.json\").write_text(json.dumps(log, indent=2), encoding=\"utf-8\")\n",
    "    print(\"\\nLog salvato in: run3_log.json\")\n",
    "    return log\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_three_steps()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
